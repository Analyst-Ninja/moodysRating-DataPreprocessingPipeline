{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f021dd-5f64-4c92-a81b-377cbb451fb3",
   "metadata": {},
   "source": [
    "#### Step 0: Defining Constants/ configurations [can be stored in seperate config.json file] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba38e6aa-ea66-4200-9005-ceef2247680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "INPUT_FILE_PATH = \"datasets/\"\n",
    "OUTPUT_FILE_PATH = \"output/\"\n",
    "\n",
    "# Data Loading conditions\n",
    "COLS_TO_DROP_DURING_LOAD = ['Current Price', 'Market Capitalization']\n",
    "FILE_TO_EXCLUDE = 'datasets/other_metrics_final.csv'\n",
    "\n",
    "# Preprocessing parameters\n",
    "MISSING_THRESHOLD = 0.1\n",
    "CORRELATION_THRESHOLD = 0.1\n",
    "\n",
    "# Preprocessing strategies\n",
    "MISSING_VALUE_STRATEGY = 'mean'  # Options: 'mean', 'median', 'most_frequent'\n",
    "\n",
    "# Target\n",
    "TARGET = 'Current Price'\n",
    "\n",
    "# Non Essential Features to drop\n",
    "COLS_TO_DROP = ['BSE Code', 'NSE Code', 'Name', 'join_key']\n",
    "\n",
    "# Test to train data fraction\n",
    "TEST_SIZE_FRACTION = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b30e22-a945-4167-9b2d-177f26c79d36",
   "metadata": {},
   "source": [
    "#### Step 1: Importing the libraries and logger set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75e990f0-7e2d-4373-b298-6421650d6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import logging\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "import config\n",
    "import glob\n",
    "from typing import List, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='log_file.log',\n",
    "    level=logging.INFO,    # Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5a0e5-b7a4-47d1-b222-8513b9fc3a11",
   "metadata": {},
   "source": [
    "#### Step 2: Defining the Preprossors (Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a2cae02-2b6d-4b16-9c46-b937fb2758db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, data: pd.DataFrame, *args) -> pd.DataFrame:\n",
    "        raise NotImplementedError(\"Subclasses must implement the 'process' method\")\n",
    "\n",
    "class DataLoader(Preprocessor):\n",
    "    def __init__(self, file_path=INPUT_FILE_PATH, cols_to_drop_during_load=None, file_to_exclude=None):\n",
    "        # Step 1: Initialize file pattern, columns to drop, and file exclusion\n",
    "        self.file_pattern = file_path + \"*\"\n",
    "        self.cols_to_drop_during_load = cols_to_drop_during_load\n",
    "        self.file_to_exclude = file_to_exclude\n",
    "        logger.info(f\"DataLoader initialized with file pattern: {self.file_pattern}\")\n",
    "\n",
    "    def drop_cols(self, file_path: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            # Step 1: Drop specified columns unless the file is excluded\n",
    "            if file_path != self.file_to_exclude:\n",
    "                logger.info(f\"Dropping columns: {self.cols_to_drop_during_load} from file: {file_path}\")\n",
    "                return df.drop(self.cols_to_drop_during_load, axis=1)\n",
    "            else:\n",
    "                logger.info(f\"Skipping column drop for file: {file_path}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in drop_cols for file {file_path}: {str(e)}\")\n",
    "\n",
    "    def process(self, *args) -> List[pd.DataFrame]:\n",
    "        try:\n",
    "            # Step 1: Load data files matching the given pattern\n",
    "            logger.info(f\"Loading data from files matching pattern: {self.file_pattern}\")\n",
    "            file_paths = glob.glob(self.file_pattern)\n",
    "            if not file_paths:\n",
    "                logger.warning(f\"No files found matching pattern: {self.file_pattern}\")\n",
    "            logger.info(f\"Found files: {file_paths}\")\n",
    "            \n",
    "            data_frames = []\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    # Process each file based on its extension\n",
    "                    file_extension = file_path[file_path.find('.') + 1 :]\n",
    "                    logger.info(f\"Processing file: {file_path} with extension: {file_extension}\")\n",
    "                    \n",
    "                    if file_extension == 'csv':\n",
    "                        # Read CSV and apply column exclusions\n",
    "                        data_frames.append(self.drop_cols(file_path=file_path, df=pd.read_csv(file_path)))\n",
    "                    elif file_extension == 'xlsx':\n",
    "                        # Read Excel and apply column exclusions\n",
    "                        data_frames.append(self.drop_cols(file_path=file_path, df=pd.read_excel(file_path)))\n",
    "                    else:\n",
    "                        # Log unsupported file types\n",
    "                        logger.error(f\"Unsupported file type: {file_extension} for file: {file_path}\")\n",
    "                        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Log errors for file processing failures\n",
    "                    logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            logger.info(f\"Successfully loaded {len(data_frames)} data frames.\")\n",
    "            return data_frames\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in DataLoader process: {str(e)}\")  \n",
    "            \n",
    "class DataJoiner(Preprocessor):\n",
    "    def __init__(self, join_type = 'inner', on_column = 'join_key'):\n",
    "        # Step 1: Initialize join type and on_column\n",
    "        self.join_type = join_type\n",
    "        self.on_column = on_column\n",
    "        logger.info(f\"DataJoiner initialized with join_type: {self.join_type}, on_column: {self.on_column}\")\n",
    "        \n",
    "    def process(self, tables: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "        # Step 1: Start the join operation with the first table and additional tables\n",
    "        data = tables[0]\n",
    "        additional_tables = tables[1:]\n",
    "        logger.info(f\"Starting join operation with {len(additional_tables)} other tables.\")\n",
    "        for idx, table in enumerate(additional_tables):\n",
    "            try:\n",
    "                # Keep only the columns not in the main table, except for the on_column\n",
    "                table = table[[col for col in table.columns if col not in data.columns or col == self.on_column]]\n",
    "                \n",
    "                # Perform the join operation on the specified column\n",
    "                data = data.merge(table, how = self.join_type, on = self.on_column)\n",
    "                logger.info(f\"Table {idx+1} successfully joined.\")\n",
    "            except:\n",
    "                # Log error if joining fails for any table\n",
    "                logger.error(f\"Error joining table {idx+1}: Error - {str(e)}\")\n",
    "                continue\n",
    "        return data\n",
    "\n",
    "class MissingValuePreprocessor(Preprocessor):\n",
    "    def __init__(self, strategy=MISSING_VALUE_STRATEGY, missing_threshold = MISSING_THRESHOLD):\n",
    "        \n",
    "        # Step 1: Initialize imputer strategy, missing threshold and imputer instance\n",
    "        self.imputer = SimpleImputer(strategy=strategy, keep_empty_features=True)\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.strategy = strategy\n",
    "        logger.info(f\"MissingValuePreprocessor initialized with strategy: {self.strategy}, missing_threshold: {self.missing_threshold}\")\n",
    "\n",
    "    def drop_columns_by_missing_threshold(self, df: pd.DataFrame, missing_threshold: float) -> pd.DataFrame:\n",
    "        try:\n",
    "            logger.info(f\"Starting column drop based on missing value threshold: {missing_threshold}\")\n",
    "            \n",
    "            # Calculate missing percentage for each column\n",
    "            missing_percentages = dict(df.isna().sum() / df.shape[0])\n",
    "            \n",
    "            # Select columns where missing percentage is within the threshold\n",
    "            cols_to_keep = [k for k, v in missing_percentages.items() if v <= missing_threshold]\n",
    "            \n",
    "            result_df = df[cols_to_keep]\n",
    "            logger.info(f\"Successfully dropped columns exceeding missing threshold\")\n",
    "            \n",
    "            return result_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in drop_columns_by_missing_threshold: {str(e)}\")\n",
    "        \n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logger.info(\"Starting missing value handling process.\")\n",
    "            \n",
    "            # Step 1: Drop columns based on missing value threshold\n",
    "            data = self.drop_columns_by_missing_threshold(df=data, missing_threshold=self.missing_threshold)\n",
    "            logger.info(f\"Remaining data after dropping columns: {data.shape[1]} columns.\")\n",
    "            \n",
    "            # Step 2: Separate numerical and categorical columns\n",
    "            numerical_cols = data.select_dtypes(include=['number']).columns\n",
    "            categorical_cols = data.select_dtypes(exclude=['number']).columns\n",
    "            \n",
    "            # Step 3: Impute missing values for numerical columns\n",
    "            num_df = pd.DataFrame(data=self.imputer.fit_transform(data[numerical_cols]), columns=numerical_cols)\n",
    "            \n",
    "            # Step 4: Handle categorical columns (no transformation shown in this snippet)\n",
    "            cat_df = pd.DataFrame(data=data, columns=categorical_cols)\n",
    "            \n",
    "            # Step 5: Combine numerical and categorical data frames\n",
    "            final_data = pd.concat([cat_df, num_df], axis=1)\n",
    "            logger.info(\"Concatenated numerical and categorical data frames.\")\n",
    "            logger.info(\"Missing value handling process completed successfully.\")\n",
    "            \n",
    "            return final_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in MissingValuePreprocessor: {str(e)}\")\n",
    "\n",
    "class ColumnRelevancePreprocessor(Preprocessor):\n",
    "    def __init__(self, \n",
    "                 correlation_threshold = CORRELATION_THRESHOLD, \n",
    "                 target = TARGET\n",
    "                ):\n",
    "        # Step 1: Initialize correlation threshold and target variable for Column Relevance checking\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.target = target\n",
    "        logger.info(f\"ColumnRelevancePreprocessor initialized with Correlation Threshold: {self.correlation_threshold}, Target Column: {self.target}\")\n",
    "\n",
    "    def extract_relevant_cols_by_target_correlation(self, df: pd.DataFrame) -> List[str]:\n",
    "        try:\n",
    "            # Step 1: Calculate correlation with target for numerical columns\n",
    "            logger.info(\"Calculating correlation of numerical columns with target.\")\n",
    "            corr_df = df.select_dtypes(include=['number']).corr()[self.target].apply(abs).reset_index()\n",
    "            corr_df.rename(columns={\"index\": \"column\"}, inplace=True)\n",
    "            \n",
    "            # Step 2: Select columns with correlation above threshold\n",
    "            logger.info(f\"Filtering columns with correlation greater than threshold: {self.correlation_threshold}\")\n",
    "            relevant_num_cols = corr_df[corr_df[self.target].apply(lambda x: x > self.correlation_threshold)]['column'].tolist()\n",
    "            logger.info(f\"Relevant numerical columns based on correlation: {relevant_num_cols}\")\n",
    "            \n",
    "            # Step 3: Get categorical columns\n",
    "            cat_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "            \n",
    "            # Step 4: Combine numerical and categorical columns\n",
    "            relevant_cols = relevant_num_cols + cat_cols\n",
    "            logger.info(f\"Total relevant columns: {len(relevant_cols)}\")\n",
    "            \n",
    "            return relevant_cols\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in extract_relevant_cols_by_target_correlation: {str(e)}\")\n",
    "\n",
    "    def process(self, data : pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logging.info('Starting process for relevant column extraction')\n",
    "            # Step 1: Calling the function for selecting relevant features \n",
    "            data = data[self.extract_relevant_cols_by_target_correlation(data)]\n",
    "            logging.info(f\"Relevant Column extracted successfully | Total relevant col : {len(data.columns)}\")\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Relevant Feature extraction : {str(e)}\")\n",
    "\n",
    "class ColumnDropper(Preprocessor):\n",
    "    def __init__(self, cols_to_drop : List[str]= None):\n",
    "        \n",
    "        self.cols_to_drop = cols_to_drop\n",
    "        logger.info(f\"ColumnDropper initialized with Columns to drop : {self.cols_to_drop}\")\n",
    "\n",
    "    def process(self, data : pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logging.info('Column Dropping Process Started')\n",
    "            # Step 1: Dropping Columns\n",
    "            data = data.drop(columns = self.cols_to_drop)\n",
    "            logging.info(f\"Columns Dropped successfully\")\n",
    "            \n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in dropping columns : {str(e)}\")\n",
    "\n",
    "class OutlierRemover(Preprocessor):\n",
    "    def __init__(self):\n",
    "        logger.info(f\"OutlierRemover initialized\")\n",
    "        pass\n",
    "\n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logging.info('Outlier Removal Process Started')\n",
    "            \n",
    "            # Step 1: Getting the Qualtiles for all columns\n",
    "            Q1 = data.select_dtypes(include=['number']).quantile(0.25)\n",
    "            Q3 = data.select_dtypes(include=['number']).quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Step 2: Calculating the lower_bound and upper_bound\n",
    "            lower_bound = Q1 - IQR * 1.5\n",
    "            upper_bound = Q3 + IQR * 1.5\n",
    "    \n",
    "            # Step 3: Getting outlier filter condition to the numeric columns\n",
    "            numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "            condition = ((data[numeric_cols] >= lower_bound) & (data[numeric_cols] <= upper_bound)).all(axis=1)\n",
    "            \n",
    "            # Step 4: Apply the same filter to both numeric and categorical columns to keep rows aligned\n",
    "            df_filtered = data[condition]\n",
    "            \n",
    "            logging.info('Outlier Removal Process completed successfully')\n",
    "            \n",
    "            return df_filtered\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Outlier Removal Process : {str(e)}\")\n",
    "            \n",
    "class OneHotEncoderPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        logger.info(f\"OneHotEncoderPreprocessor initialized\")\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        logger.info(f\"OneHotEncoder initialized\")\n",
    "\n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logging.info('One Hot Encoding Process started')\n",
    "            # Step 1: Separate numerical and categorical columns\n",
    "            logger.info(\"Separating numerical and categorical columns.\")\n",
    "            num_df = data.select_dtypes(include=['number']).reset_index(drop=True)\n",
    "            cat_df = data.select_dtypes(exclude=['number']).reset_index(drop=True)\n",
    "            logger.info(f\"Identified {len(num_df.columns)} numerical and {len(cat_df.columns)} categorical columns.\")\n",
    "            \n",
    "            # Step 2: Encode categorical columns\n",
    "            logger.info(\"Encoding categorical columns using the encoder.\")\n",
    "            encoded_data = self.encoder.fit_transform(cat_df)\n",
    "            encoded_columns = self.encoder.get_feature_names_out(cat_df.columns)\n",
    "            \n",
    "            # Step 3: Create DataFrame from encoded data\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)\n",
    "            \n",
    "            # Step 4: Concatenate numerical and encoded categorical data\n",
    "            logger.info(\"Concatenating numerical and encoded categorical data frames.\")\n",
    "            final_data = pd.concat([num_df, encoded_df], axis=1)\n",
    "            logger.info(f\"Final data frame created with {final_data.shape[1]} columns.\")\n",
    "            logging.info('One Hot Encoding Process completed successfully')\n",
    "            \n",
    "            return final_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in one hot encoding process: {str(e)}\")\n",
    "\n",
    "class StandardizationPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        logger.info(f\"StandardizationPreprocessor initialized\")\n",
    "        self.scaler = StandardScaler()\n",
    "        logger.info(f\"Scaler initialized\")\n",
    "\n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            logging.info('Standardization Process Started')\n",
    "            # Step 1: Separate numerical and categorical columns\n",
    "            logger.info(\"Identifying numerical and categorical columns.\")\n",
    "            numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "            num_df = data.select_dtypes(include=['number'])\n",
    "            cat_df = data.select_dtypes(exclude=['number']).reset_index(drop=True)\n",
    "            logger.info(f\"Found {len(numeric_cols)} numerical columns and {cat_df.shape[1]} categorical columns.\")\n",
    "            \n",
    "            # Step 2: Scale numerical columns\n",
    "            logger.info(\"Scaling numerical columns.\")\n",
    "            scaled_data = self.scaler.fit_transform(num_df)\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols)\n",
    "            logger.info(\"Scaling completed for numerical columns.\")\n",
    "            \n",
    "            # Step 3: Concatenate scaled numerical and categorical data\n",
    "            logger.info(\"Concatenating scaled numerical and categorical columns.\")\n",
    "            final_data = pd.concat([scaled_df, cat_df], axis=1)\n",
    "            logger.info(f\"Final data frame created with {final_data.shape[1]} columns.\")\n",
    "            logging.info('Standardization Process completed successfully')\n",
    "            \n",
    "            return final_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data scaling process: {str(e)}\")\n",
    "\n",
    "class TrainTestSplitter(Preprocessor):\n",
    "    def __init__(self, test_size = 0.2, random_state = 42, target = TARGET):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.target = target\n",
    "        logger.info(f\"TrainTestSplitter initialized with Test Size : {self.test_size} | Target Variable : {self.target} | Random State : {self.random_state}\")\n",
    "\n",
    "    def process(self, data: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "        try:\n",
    "            logging.info('Train Test split process started successfully')\n",
    "            # Step 1: Separate independent and dependent variables\n",
    "            logger.info(\"Separating independent and dependent variables.\")\n",
    "            independent_var_df = data[[x for x in data.columns if x != self.target]]\n",
    "            dependent_var_df = data[[self.target]]\n",
    "            logger.info(f\"Independent variables: {independent_var_df.shape[1]} columns, Dependent variable: {dependent_var_df.shape[1]} columns.\")\n",
    "            \n",
    "            # Step 2: Split the data into training and testing sets\n",
    "            logger.info(f\"Splitting data into training and testing sets with test size = {self.test_size}.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                independent_var_df,\n",
    "                dependent_var_df,\n",
    "                test_size=self.test_size,\n",
    "                random_state=self.random_state)\n",
    "            logger.info(f\"Data split completed. Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}.\")\n",
    "            \n",
    "            logging.info('Train Test split process started completed successfully')\n",
    "            \n",
    "            # Step 3: Return the split datasets\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during the data splitting process: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe1386-957a-4b5d-ab77-a05007d0d2d9",
   "metadata": {},
   "source": [
    "#### Step 3: Defining Preprocessor Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fb6d46d-f367-4c8b-88e2-9a1099a4fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessorFactory:\n",
    "    @staticmethod\n",
    "    def get_preprocessor(preprocessor_type: str, **kwargs) -> Preprocessor:\n",
    "        try:\n",
    "            # Checking for preprocessor type and return the required Preprocessor Class\n",
    "            if preprocessor_type == 'data_loader':\n",
    "                return DataLoader(**kwargs)\n",
    "            elif preprocessor_type == 'data_joiner':\n",
    "                return DataJoiner(**kwargs)\n",
    "            elif preprocessor_type == 'missing_values':\n",
    "                return MissingValuePreprocessor(**kwargs)\n",
    "            elif preprocessor_type == 'feature_selection':\n",
    "                return ColumnRelevancePreprocessor(**kwargs)\n",
    "            elif preprocessor_type == 'drop_columns':\n",
    "                return ColumnDropper(**kwargs)\n",
    "            elif preprocessor_type == 'outlier_removal':\n",
    "                return OutlierRemover(**kwargs)\n",
    "            elif preprocessor_type == 'one_hot_encoder':\n",
    "                return OneHotEncoderPreprocessor(**kwargs)\n",
    "            elif preprocessor_type == 'feature_scaler':\n",
    "                return StandardizationPreprocessor(**kwargs)\n",
    "            elif preprocessor_type == 'train_test_split':\n",
    "                return TrainTestSplitter(**kwargs)\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Preprocessor Not Defined | Error : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55493536-feb6-4872-8d2d-488560e2640f",
   "metadata": {},
   "source": [
    "#### Step 4: Defining DataPreprocessingPipeline Class to execute the steps sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22d311ee-a89f-4a4b-bbc4-301ca3e9c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessingPipeline:\n",
    "    def __init__(self):\n",
    "        logging.info('Data Processing Pipeline Initialized')\n",
    "        self.steps = []\n",
    "\n",
    "    def add_step(self, step: Preprocessor):\n",
    "        try:\n",
    "            # Step 1: Adding Step to Pipeline\n",
    "            self.steps.append(step)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error in adding step to pipeline: {str(e)}')\n",
    "\n",
    "    def execute(self, initial_data: pd.DataFrame = None) -> Tuple[pd.DataFrame]:\n",
    "        try:\n",
    "            logging.info('Pipeline Execution Started')\n",
    "            # Step 1: Giving the initial data as None to get the process started\n",
    "            data = initial_data\n",
    "            for step in self.steps:\n",
    "                data = step.process(data)\n",
    "\n",
    "            logging.info('Pipeline Execution Completed Successfully.')\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f'Error occured in Pipeline Execution : {str(e)}')\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691172ea-3620-4b8b-95a6-a60af0962ec0",
   "metadata": {},
   "source": [
    "#### Step 5: Execution of Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66f7a909-016d-4c6c-ace6-a53ee263fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Instantiating the DataPreprocessingPipeline Class\n",
    "pipeline = DataPreprocessingPipeline()\n",
    "\n",
    "# Step 2: Adding the steps to Pipeline with required parameters\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor(\n",
    "    'data_loader', \n",
    "    file_path = INPUT_FILE_PATH,\n",
    "    cols_to_drop_during_load = COLS_TO_DROP_DURING_LOAD,\n",
    "    file_to_exclude = FILE_TO_EXCLUDE\n",
    "))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor('data_joiner', join_type = 'inner', on_column = 'join_key'))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor('drop_columns', cols_to_drop = COLS_TO_DROP))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor(\n",
    "    'missing_values', \n",
    "    strategy = MISSING_VALUE_STRATEGY,\n",
    "    missing_threshold = MISSING_THRESHOLD\n",
    "))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor(\n",
    "    'feature_selection', \n",
    "    correlation_threshold = CORRELATION_THRESHOLD, \n",
    "    target = TARGET\n",
    "))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor('outlier_removal'))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor('feature_scaler'))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor('one_hot_encoder'))\n",
    "pipeline.add_step(PreprocessorFactory.get_preprocessor(\n",
    "    'train_test_split', \n",
    "    test_size = TEST_SIZE_FRACTION, \n",
    "    random_state = 234,\n",
    "    target = TARGET\n",
    "))\n",
    "\n",
    "# Step 3: Pipeline Execution\n",
    "processed_data = pipeline.execute()\n",
    "\n",
    "# Step 4: Extrating the Training and Test data for ML algorithm\n",
    "X_train, X_test, y_train, y_test = processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10564ae2-925b-4b88-a399-43b1b222013f",
   "metadata": {},
   "source": [
    "#### Step 6: Checking the shape of dfs for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99295ec1-32e8-408e-bb45-a7d880ed3c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2230, 103), (394, 103), (2230, 1), (394, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd90440d-20a1-408d-9c9d-e0baf2e1df72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>52w Index</th>\n",
       "      <th>Down from 52w high</th>\n",
       "      <th>From 52w high</th>\n",
       "      <th>NCAVPS</th>\n",
       "      <th>Market Capitalization</th>\n",
       "      <th>Piotroski score</th>\n",
       "      <th>Public holding</th>\n",
       "      <th>FII holding</th>\n",
       "      <th>DII holding</th>\n",
       "      <th>EPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Industry_Textiles - Jute - Yarn / Products</th>\n",
       "      <th>Industry_Textiles - Manmade</th>\n",
       "      <th>Industry_Textiles - Processing</th>\n",
       "      <th>Industry_Textiles - Products</th>\n",
       "      <th>Industry_Textiles - Spinning - Synthetic / Blended</th>\n",
       "      <th>Industry_Trading</th>\n",
       "      <th>Industry_Transmisson Line Towers / Equipment</th>\n",
       "      <th>Industry_Transport - Airlines</th>\n",
       "      <th>Industry_Travel Agencies</th>\n",
       "      <th>Industry_Tyres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>-0.085094</td>\n",
       "      <td>-0.484143</td>\n",
       "      <td>0.475923</td>\n",
       "      <td>-0.218499</td>\n",
       "      <td>-0.498459</td>\n",
       "      <td>0.489071</td>\n",
       "      <td>-0.537281</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>-1.384755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.310741</td>\n",
       "      <td>-0.244670</td>\n",
       "      <td>0.274773</td>\n",
       "      <td>-0.087416</td>\n",
       "      <td>-0.182986</td>\n",
       "      <td>-1.070882</td>\n",
       "      <td>0.252610</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.332548</td>\n",
       "      <td>-0.409315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>-1.061394</td>\n",
       "      <td>1.367919</td>\n",
       "      <td>-1.334428</td>\n",
       "      <td>-0.234026</td>\n",
       "      <td>-0.498677</td>\n",
       "      <td>-0.030914</td>\n",
       "      <td>1.122269</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>-0.473489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>-0.504421</td>\n",
       "      <td>1.149911</td>\n",
       "      <td>-1.133278</td>\n",
       "      <td>-0.757461</td>\n",
       "      <td>2.204389</td>\n",
       "      <td>2.049024</td>\n",
       "      <td>0.509909</td>\n",
       "      <td>-0.095686</td>\n",
       "      <td>1.473717</td>\n",
       "      <td>-0.422150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>0.827273</td>\n",
       "      <td>-0.508963</td>\n",
       "      <td>0.475923</td>\n",
       "      <td>-0.568451</td>\n",
       "      <td>-0.420512</td>\n",
       "      <td>1.009055</td>\n",
       "      <td>-0.488100</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>0.862927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>0.924302</td>\n",
       "      <td>-1.052306</td>\n",
       "      <td>1.079373</td>\n",
       "      <td>-0.799861</td>\n",
       "      <td>-0.519241</td>\n",
       "      <td>-0.550898</td>\n",
       "      <td>2.522719</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>-1.084743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>-0.047110</td>\n",
       "      <td>-0.147405</td>\n",
       "      <td>0.140673</td>\n",
       "      <td>-0.536800</td>\n",
       "      <td>5.968943</td>\n",
       "      <td>0.489071</td>\n",
       "      <td>-1.342765</td>\n",
       "      <td>4.757057</td>\n",
       "      <td>-0.297131</td>\n",
       "      <td>-0.390063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-0.034323</td>\n",
       "      <td>0.397280</td>\n",
       "      <td>-0.395727</td>\n",
       "      <td>-0.621601</td>\n",
       "      <td>-0.212452</td>\n",
       "      <td>0.489071</td>\n",
       "      <td>1.779611</td>\n",
       "      <td>-0.337225</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>-0.436589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>-0.501413</td>\n",
       "      <td>-0.066909</td>\n",
       "      <td>0.073623</td>\n",
       "      <td>-0.712075</td>\n",
       "      <td>-0.492030</td>\n",
       "      <td>-1.070882</td>\n",
       "      <td>0.266404</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>-0.356160</td>\n",
       "      <td>-0.507180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>1.666681</td>\n",
       "      <td>-1.616444</td>\n",
       "      <td>1.615773</td>\n",
       "      <td>-0.822853</td>\n",
       "      <td>0.695549</td>\n",
       "      <td>1.529039</td>\n",
       "      <td>-2.355168</td>\n",
       "      <td>-0.348204</td>\n",
       "      <td>1.509134</td>\n",
       "      <td>-0.418941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2230 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      52w Index  Down from 52w high  From 52w high    NCAVPS  \\\n",
       "619   -0.085094           -0.484143       0.475923 -0.218499   \n",
       "297   -0.310741           -0.244670       0.274773 -0.087416   \n",
       "930   -1.061394            1.367919      -1.334428 -0.234026   \n",
       "899   -0.504421            1.149911      -1.133278 -0.757461   \n",
       "1565   0.827273           -0.508963       0.475923 -0.568451   \n",
       "...         ...                 ...            ...       ...   \n",
       "1185   0.924302           -1.052306       1.079373 -0.799861   \n",
       "863   -0.047110           -0.147405       0.140673 -0.536800   \n",
       "324   -0.034323            0.397280      -0.395727 -0.621601   \n",
       "1951  -0.501413           -0.066909       0.073623 -0.712075   \n",
       "2504   1.666681           -1.616444       1.615773 -0.822853   \n",
       "\n",
       "      Market Capitalization  Piotroski score  Public holding  FII holding  \\\n",
       "619               -0.498459         0.489071       -0.537281    -0.348204   \n",
       "297               -0.182986        -1.070882        0.252610    -0.348204   \n",
       "930               -0.498677        -0.030914        1.122269    -0.348204   \n",
       "899                2.204389         2.049024        0.509909    -0.095686   \n",
       "1565              -0.420512         1.009055       -0.488100    -0.348204   \n",
       "...                     ...              ...             ...          ...   \n",
       "1185              -0.519241        -0.550898        2.522719    -0.348204   \n",
       "863                5.968943         0.489071       -1.342765     4.757057   \n",
       "324               -0.212452         0.489071        1.779611    -0.337225   \n",
       "1951              -0.492030        -1.070882        0.266404    -0.348204   \n",
       "2504               0.695549         1.529039       -2.355168    -0.348204   \n",
       "\n",
       "      DII holding       EPS  ...  Industry_Textiles - Jute - Yarn / Products  \\\n",
       "619     -0.356160 -1.384755  ...                                         0.0   \n",
       "297     -0.332548 -0.409315  ...                                         0.0   \n",
       "930     -0.356160 -0.473489  ...                                         0.0   \n",
       "899      1.473717 -0.422150  ...                                         0.0   \n",
       "1565    -0.356160  0.862927  ...                                         0.0   \n",
       "...           ...       ...  ...                                         ...   \n",
       "1185    -0.356160 -1.084743  ...                                         0.0   \n",
       "863     -0.297131 -0.390063  ...                                         0.0   \n",
       "324     -0.356160 -0.436589  ...                                         0.0   \n",
       "1951    -0.356160 -0.507180  ...                                         0.0   \n",
       "2504     1.509134 -0.418941  ...                                         0.0   \n",
       "\n",
       "      Industry_Textiles - Manmade  Industry_Textiles - Processing  \\\n",
       "619                           0.0                             0.0   \n",
       "297                           0.0                             0.0   \n",
       "930                           0.0                             0.0   \n",
       "899                           0.0                             0.0   \n",
       "1565                          0.0                             0.0   \n",
       "...                           ...                             ...   \n",
       "1185                          0.0                             0.0   \n",
       "863                           0.0                             0.0   \n",
       "324                           0.0                             0.0   \n",
       "1951                          0.0                             0.0   \n",
       "2504                          0.0                             0.0   \n",
       "\n",
       "      Industry_Textiles - Products  \\\n",
       "619                            0.0   \n",
       "297                            0.0   \n",
       "930                            0.0   \n",
       "899                            0.0   \n",
       "1565                           0.0   \n",
       "...                            ...   \n",
       "1185                           0.0   \n",
       "863                            0.0   \n",
       "324                            1.0   \n",
       "1951                           0.0   \n",
       "2504                           0.0   \n",
       "\n",
       "      Industry_Textiles - Spinning - Synthetic / Blended  Industry_Trading  \\\n",
       "619                                                 0.0                0.0   \n",
       "297                                                 0.0                0.0   \n",
       "930                                                 0.0                0.0   \n",
       "899                                                 0.0                0.0   \n",
       "1565                                                0.0                1.0   \n",
       "...                                                 ...                ...   \n",
       "1185                                                0.0                0.0   \n",
       "863                                                 0.0                0.0   \n",
       "324                                                 0.0                0.0   \n",
       "1951                                                0.0                1.0   \n",
       "2504                                                0.0                0.0   \n",
       "\n",
       "      Industry_Transmisson Line Towers / Equipment  \\\n",
       "619                                            0.0   \n",
       "297                                            0.0   \n",
       "930                                            0.0   \n",
       "899                                            0.0   \n",
       "1565                                           0.0   \n",
       "...                                            ...   \n",
       "1185                                           0.0   \n",
       "863                                            0.0   \n",
       "324                                            0.0   \n",
       "1951                                           0.0   \n",
       "2504                                           0.0   \n",
       "\n",
       "      Industry_Transport - Airlines  Industry_Travel Agencies  Industry_Tyres  \n",
       "619                             0.0                       0.0             0.0  \n",
       "297                             0.0                       0.0             0.0  \n",
       "930                             0.0                       0.0             0.0  \n",
       "899                             0.0                       0.0             0.0  \n",
       "1565                            0.0                       0.0             0.0  \n",
       "...                             ...                       ...             ...  \n",
       "1185                            0.0                       0.0             0.0  \n",
       "863                             0.0                       0.0             0.0  \n",
       "324                             0.0                       0.0             0.0  \n",
       "1951                            0.0                       0.0             0.0  \n",
       "2504                            0.0                       0.0             0.0  \n",
       "\n",
       "[2230 rows x 103 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535d1a4-5d46-4a1d-ab1a-d87fb5d1a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904a1a8-8903-411b-bddd-9b5ffee54bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
